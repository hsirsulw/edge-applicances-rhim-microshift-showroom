= Module 3: Deploying to edge locations
:page-nav-title: Deploy & Verify Appliance
:icons: font
:source-highlighter: rouge
:experimental:

== Learning objectives
By the end of this module, you will be able to:

* [x] Create an isolated network for disconnected environment simulation
* [x] Deploy a bootc image to a virtual machine using KVM
* [x] Configure kubeconfig for MicroShift access
* [x] Verify a self-contained appliance is running correctly
* [x] Confirm embedded container images are being used (not pulled from network)

[#know_section]
[#deploying]
== 3.1 Know: Deploying to Edge Locations

=== The Business Challenge
ManufacturingCo must deploy appliances to 50 factory locations with varying constraints.

[cols="1,2a", options="header"]
|===
| Constraint | Impact on Deployment
| **No Connectivity** | Standard "pull-based" cloud deployments will fail instantly.
| **Low Bandwidth** | Large image downloads would take days or disrupt factory operations.
| **Limited On-site IT** | The installation process must be "Zero Touch" and automated.
|===

**The Solution:** ISO-based deployment provides a single, portable file that contains the entire OS, MicroShift, and application stack.

=== ISO-Based Deployment vs. Traditional Methods

[cols="1a,1a", options="header"]
|===
| ❌ Traditional Network-Dependent | ✅ Image Mode ISO (Disconnected)
| 
* Requires internet to pull packages/images.
* Fragile: Installation fails if network drops.
* Configuration drift across different sites.
| 
* **Self-Contained:** Everything is inside the ISO.
* **Resilient:** Works via USB or local PXE boot.
* **Atomic:** Identical installation at every site.
|===

[#show_section]
[#boot_vm]
== 3.2 Show: Deploy the Appliance to Target System

=== The Deployment Workflow
We are simulating the "Air-Gapped Factory" by creating an isolated virtual network before installing the ISO.

[cols="^1,2,2", options="header"]
|===
| Step | Action | Purpose
| 1 | **Isolate Network** | Simulate a factory with no internet access.
| 2 | **Define VM** | Prepare the target hardware (simulated via KVM).
| 3 | **Automate Install** | Use Kickstart to install the image with zero manual clicks.
|===

=== 1. Create Isolated Network
. View the setup script:
+
[source,sh,role=execute]
----
cat setup-isolated-network.sh
----
+
[%collapsible]
====
.Output of `setup-isolated-network.sh`
[source,bash]
----
#!/bin/bash
#
# Script to create an isolated KVM network (no internet access)
# and optionally attach a VM to it
#

set -e

# Configuration
NETWORK_NAME="bootc-isolated"
BRIDGE_NAME="virbr-bootc"
NETWORK_IP="192.168.100.1"
NETWORK_NETMASK="255.255.255.0"
NETWORK_RANGE_START="192.168.100.2"
NETWORK_RANGE_END="192.168.100.254"
NETWORK_XML="/tmp/${NETWORK_NAME}.xml"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Function to print colored output
print_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

print_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Check if running as root or with sudo
if [ "$EUID" -ne 0 ]; then 
    print_error "Please run as root or with sudo"
    exit 1
fi

# Check if network already exists (defined or active)
if virsh net-info "${NETWORK_NAME}" &>/dev/null || virsh net-list --all --name | grep -q "^${NETWORK_NAME}$"; then
    print_warn "Network '${NETWORK_NAME}' already exists"
    read -p "Do you want to destroy and recreate it? (y/N): " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        print_info "Destroying existing network..."
        # Try to destroy if active
        if virsh net-destroy "${NETWORK_NAME}" &>/dev/null; then
            print_info "Network destroyed"
        fi
        # Undefine the network
        if virsh net-undefine "${NETWORK_NAME}" &>/dev/null; then
            print_info "Network undefined"
        fi
    else
        print_info "Exiting without changes"
        exit 0
    fi
fi

# Create network XML file
print_info "Creating isolated network XML definition..."
cat > "${NETWORK_XML}" <<EOF
<network>
  <name>${NETWORK_NAME}</name>
  <uuid>$(uuidgen)</uuid>
  <forward mode='none'/>
  <bridge name='${BRIDGE_NAME}' stp='on' delay='0'/>
  <ip address='${NETWORK_IP}' netmask='${NETWORK_NETMASK}'>
    <dhcp>
      <range start='${NETWORK_RANGE_START}' end='${NETWORK_RANGE_END}'/>
    </dhcp>
  </ip>
</network>
EOF

print_info "Network XML created at ${NETWORK_XML}"

# Define the network
print_info "Defining network '${NETWORK_NAME}'..."
virsh net-define "${NETWORK_XML}"

# Start the network
print_info "Starting network '${NETWORK_NAME}'..."
virsh net-start "${NETWORK_NAME}"

# Set network to autostart
print_info "Setting network '${NETWORK_NAME}' to autostart..."
virsh net-autostart "${NETWORK_NAME}"

# Verify network is active
if virsh net-info "${NETWORK_NAME}" | grep -q "Active:.*yes"; then
    print_info "Network '${NETWORK_NAME}' is now active and isolated (no internet access)"
    print_info "Network details:"
    virsh net-info "${NETWORK_NAME}"
    echo
    print_info "Network IP range: ${NETWORK_RANGE_START} - ${NETWORK_RANGE_END}"
    print_info "Gateway/DNS: ${NETWORK_IP}"
else
    print_error "Failed to start network"
    exit 1
fi

# Clean up temporary XML file
rm -f "${NETWORK_XML}"

print_info "Isolated network setup complete!"
print_info "To use this network in create-vm.sh, set NETNAME='${NETWORK_NAME}'"
----
====

. Create the network:
+
[source,sh,role=execute]
----
sudo bash setup-isolated-network.sh
----

=== 2. Create and Boot the VM
We use a **Kickstart** file to automate the partitioning and `bootc` installation. This ensures the appliance is ready for production as soon as the first boot completes.

. View the VM creation script:
+
[source,sh,role=execute]
----
cat create-vm.sh
----
+
[%collapsible]
====
.Output of `create-vm.sh`
[source,bash]
----
VMNAME=microshift-4.19-bootc-vm1
NETNAME=bootc-isolated
ISO_FILE="microshift-bootc-embedded-4.19.iso"
LIBVIRT_IMAGES_DIR="/var/lib/libvirt/images"
KS_FILE="kickstart.ks"

# Get the directory where the script is located or use current directory
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
SOURCE_ISO="${SCRIPT_DIR}/${ISO_FILE}"
TARGET_ISO="${LIBVIRT_IMAGES_DIR}/${ISO_FILE}"
SOURCE_KS="${SCRIPT_DIR}/${KS_FILE}"

# Check if source ISO file exists
if [ ! -f "${SOURCE_ISO}" ]; then
    echo "Error: ISO file not found at ${SOURCE_ISO}"
    echo "Please ensure the ISO file exists or update ISO_FILE variable"
    exit 1
fi

# Check if running as root or with sudo
if [ "$EUID" -ne 0 ]; then 
    echo "Error: Please run as root or with sudo"
    exit 1
fi

# Ensure libvirt images directory exists
mkdir -p "${LIBVIRT_IMAGES_DIR}"

# Copy ISO to libvirt images directory if it doesn't exist or is different
if [ ! -f "${TARGET_ISO}" ] || [ "${SOURCE_ISO}" -nt "${TARGET_ISO}" ]; then
    echo "Copying ISO to ${TARGET_ISO} (this may take a while for large files)..."
    cp "${SOURCE_ISO}" "${TARGET_ISO}"
    chmod 644 "${TARGET_ISO}"
    echo "ISO copied successfully"
else
    echo "ISO already exists at ${TARGET_ISO}, skipping copy"
fi

# Create kickstart file with LVM partitioning
echo "Creating kickstart file with LVM partitioning..."
cat > "${SOURCE_KS}" <<'EOFKS'
lang en_US.UTF-8
keyboard us
timezone UTC
text
reboot

# Partition the disk with hardware-specific boot and swap partitions, adding an
# LVM volume that contains a 10GB+ system root. The remainder of the volume will
# be used by the CSI driver for storing data.
zerombr
clearpart --all --initlabel
# Create boot and swap partitions as required by the current hardware platform
reqpart --add-boot
# Add an LVM volume group and allocate a system root logical volume
part pv.01 --grow
volgroup rhel pv.01
logvol / --vgname=rhel --fstype=xfs --size=50240 --name=root

# Lock root user account
rootpw --lock

# Configure network to use DHCP and activate on boot
network --bootproto=dhcp --device=link --activate --onboot=on

# Configure bootc to install from the local embedded container repository.
# See /osbuild-base.ks on ISO images generated by bootc-image-builder.
ostreecontainer --transport oci --url /run/install/repo/container

%post --log=/dev/console --erroronfail
%end
EOFKS
echo "Kickstart file created at ${SOURCE_KS}"

# Create the VM using location with kernel/initrd from ISO
virt-install --name ${VMNAME} \
--os-variant fedora-coreos-stable \
--memory 8192 \
--vcpus 4 \
--disk size=120 \
--network network=${NETNAME} \
--location "${TARGET_ISO},kernel=images/pxeboot/vmlinuz,initrd=images/pxeboot/initrd.img" \
--initrd-inject "${SOURCE_KS}" \
--extra-args "inst.ks=file:/kickstart.ks console=ttyS0" \
--serial pty \
--console pty,target_type=serial \
--wait

# Clean up temporary kickstart file
rm -f "${SOURCE_KS}"
----
====

. Execute the deployment:
+
[source,sh,role=execute]
----
sudo bash create-vm.sh
----

. Watch the installation (Optional):
+
[source,sh,role=execute]
----
sudo virsh console microshift-4.19-bootc-vm1
----

[TIP]
.Architecture Insight: Kickstart & bootc
====
The Kickstart file uses the `ostreecontainer` command to tell the installer to pull the OS directly from the local OCI archive embedded on the ISO, rather than a remote registry.
====

=== 3. Verify VM Status
[source,sh,role=execute]
----
sudo virsh list --all | grep microshift-4.19-bootc-vm1
----

[#verify_disconnected]
== 3.3 Show: Verify Disconnected Operation

=== Connectivity & Access
Now we prove the appliance functions without any external network reach.

. Find the IP address of the VM:
+
[source,sh,role=execute]
----
sudo virsh domifaddr microshift-4.19-bootc-vm1
----
+
The output will show the IP address assigned to the VM. Note the IP address (for example, `192.168.100.165`).

. Connect to the VM via SSH using the IP address:
+
[source,sh,role=execute]
----
ssh redhat@<VM_IP_ADDRESS>
----
+
When prompted, enter the password: `redhat02`
+
[NOTE]
====
The default user is `redhat` with password `redhat02`. You should see a message indicating "Boot Status is GREEN - Health Check SUCCESS" when you successfully connect.
====

. **Check System Health:**
+
[source,sh,role=execute]
----
sudo bootc status
----

. **Configure MicroShift (kubeconfig):**
+
[source,sh,role=execute]
----
mkdir -p ~/.kube
sudo cat /var/lib/microshift/resources/kubeadmin/kubeconfig > ~/.kube/config
chmod go-r ~/.kube/config
----

=== Verification: Proving the "Air-Gap"
The critical proof of success is verifying that images were **not** pulled from the internet.

. **Verify Application Readiness:**
+
[source,sh,role=execute]
----
oc get nodes
oc get pods -A
----

. **The "Smoking Gun" Verification:**
Check the Kubernetes events. You are looking for the message: **"Container image already present on machine"**.
+
[source,sh,role=execute]
----
oc get events -A | grep -i pull
----

[IMPORTANT]
====
If you see **"Successfully pulled"**, the system used the internet. If you see **"already present"**, you have successfully validated a disconnected, image-mode appliance!
====

[#validation]
== 3.4 Validation: Appliance Operating Successfully

=== Expected Results Checklist
[cols="1,2", options="header"]
|===
| Test | Expected Result
| **Boot Status** | `bootc status` shows 4.19 active.
| **Node State** | `Ready` state in `oc get nodes`.
| **App Status** | WordPress and MySQL pods are `Running`.
| **Offline Proof** | `crictl images` shows all images; Events show `already present`.
|===

=== Exploring Application Manifests (Optional)
To see how MicroShift manages the embedded workloads:
[source,sh,role=execute]
----
ls /usr/lib/microshift/manifests.d/wordpress
----

[NOTE]
====
Note the image reference `docker.io/library/wordpress:6.2.1-apache`. Even though it looks like a remote URL, MicroShift is redirected to the **internal local storage** created during the image build.
====

---
**Next:** Proceed to xref:module-04.adoc[Module 4: Safe Updates & Rollbacks]
